{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "[E-04]_Create_Lyricist_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Lyricist for Lovers"
      ],
      "metadata": {
        "id": "f_vXxvDM0HzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal of this project\r\n",
        "- Create a text generator with lyrics textfiles dataset.\r\n",
        "- Perform Data cleaning in proper ways.\r\n",
        "- Get an acceptable validation loss of the text generator model, lower then 2.2."
      ],
      "metadata": {
        "id": "9me5DlDi40iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "WMM4H0Ga2tCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "lyrics_path = '/content/drive/MyDrive/aiffel/EXP_4_data/lyrics/'"
      ],
      "outputs": [],
      "metadata": {
        "id": "to2P0qV1z99O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import os, re, glob\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# open the file in read mode\r\n",
        "# read the data as a list, line by line\r\n",
        "file_paths = glob.glob(lyrics_path + '*.txt')\r\n",
        "raw_corpus = []\r\n",
        "for textfile in file_paths:\r\n",
        "  with open(textfile, \"r\") as f:\r\n",
        "      raw_corpus.extend(f.read().splitlines())\r\n",
        "\r\n",
        "print(raw_corpus[:9])\r\n",
        "print(len(raw_corpus))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Looking for some education', 'Made my way into the night', 'All that bullshit conversation', \"Baby, can't you read the signs? I won't bore you with the details, baby\", \"I don't even wanna waste your time\", \"Let's just say that maybe\", 'You could help me ease my mind', \"I ain't Mr. Right But if you're looking for fast love\", \"If that's love in your eyes\"]\n",
            "187088\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrzBaPcR2voC",
        "outputId": "620cbe55-baf1-413c-a024-bc417b5f40f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the Text Data"
      ],
      "metadata": {
        "id": "msFjDecS4h7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove the special characters"
      ],
      "metadata": {
        "id": "n415zpAV6WQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convert to lowercase, remove spaces on both sides\n",
        "2. Put a space on either side of the special character\n",
        "3. Replace multiple spaces with a single space\n",
        "4. Replace all characters other than a-zA-Z?.!,¿ with a single space\n",
        "5. Erase both spaces again\n",
        "6. Add <start> at the beginning of the statement and <end> at the end\n"
      ],
      "metadata": {
        "id": "RudoN3fC6rki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "def preprocess_sentence(sentence):\r\n",
        "    sentence = sentence.lower().strip() # 1\r\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\r\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\r\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\r\n",
        "    sentence = sentence.strip() # 5\r\n",
        "    sentence = '<start> ' + sentence + ' <end>' # 6\r\n",
        "    return sentence\r\n",
        "\r\n",
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQhGeA1F6cs5",
        "outputId": "468117d2-469f-4240-d115-19a9895a2a66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to clean the texts to put them in the traning model."
      ],
      "metadata": {
        "id": "u2U-MTAn6SNC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "def preprocess_add(raw_corpus):\r\n",
        "  corpus = []\r\n",
        "  for idx, sentence in enumerate(raw_corpus):\r\n",
        "    if len(sentence) == 0: continue   # skip if the length is 0\r\n",
        "    if sentence[-1] == \":\": continue  # skip if the text ends with \":\"\r\n",
        "  \r\n",
        "    corpus.append(preprocess_sentence(sentence))\r\n",
        "  return corpus\r\n",
        "\r\n",
        "    # if idx > 9: break   \r\n",
        "        \r\n",
        "    # print(sentence)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8ZX_02Tq4kWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "corpus = preprocess_add(raw_corpus)\r\n",
        "print(len(corpus))\r\n",
        "print(corpus[:9])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "175749\n",
            "['<start> looking for some education <end>', '<start> made my way into the night <end>', '<start> all that bullshit conversation <end>', '<start> baby , can t you read the signs ? i won t bore you with the details , baby <end>', '<start> i don t even wanna waste your time <end>', '<start> let s just say that maybe <end>', '<start> you could help me ease my mind <end>', '<start> i ain t mr . right but if you re looking for fast love <end>', '<start> if that s love in your eyes <end>']\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDoBTttl74n0",
        "outputId": "3964054b-e499-40b8-e9ef-754ef1205be4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the special characeters are gone, it looks clean now."
      ],
      "metadata": {
        "id": "STbpLCOT8Jtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, remove lines if the number of words is bigger then 15"
      ],
      "metadata": {
        "id": "4DIAGTGaEIWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "reduced_corpus = [line for line in corpus if len(line.split(' ')) <= 15]\r\n",
        "print(len(reduced_corpus))\r\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156013\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUMRVqh8EZPC",
        "outputId": "e3b9ea3c-4c9f-44d8-a754-0f99550b3fb5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "949HYQnF6Zi-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "def tokenize(corpus):\r\n",
        "    # create a tokenizer that remembers 7000 words.\r\n",
        "    # no need filters any more since we already did it above.\r\n",
        "    # replace with '<unk>' if the words not inluded in the 7000\r\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "        num_words=7000, \r\n",
        "        filters=' ',\r\n",
        "        oov_token=\"<unk>\"\r\n",
        "    )\r\n",
        "    # cinokete dictionary of tokenizer using corpus\r\n",
        "    tokenizer.fit_on_texts(corpus)\r\n",
        "    # Transofrm corpus to Tensor using the tokenizer\r\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)   \r\n",
        "\r\n",
        "    # Set the sequence length of the input data to be constant\r\n",
        "    # If the sequence is short, add padding to the end of the sentence to match the length.\r\n",
        "    # Use padding='pre' if you want to add padding to the front of the sentence to match the length\r\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  # set maxlen to 15 to set the length of the sequence.\r\n",
        "    \r\n",
        "    print(tensor,tokenizer)\r\n",
        "    return tensor, tokenizer\r\n",
        "\r\n",
        "tensor, tokenizer = tokenize(reduced_corpus)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  2 290  28 ...   0   0   0]\n",
            " [  2 219  13 ...   0   0   0]\n",
            " [  2  25  15 ...   0   0   0]\n",
            " ...\n",
            " [  2  21  77 ...   0   0   0]\n",
            " [  2  41  26 ...   0   0   0]\n",
            " [  2  21  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f21a1743e50>\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlhT31MZ8PB_",
        "outputId": "0e172c70-09b9-40b8-827f-dba93c4d6786"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check how the dictionary is formatted"
      ],
      "metadata": {
        "id": "cJ8L4x4o-G0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "for idx in tokenizer.index_word:\r\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\r\n",
        "\r\n",
        "    if idx >= 10: break"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU_Rtz6B-KxF",
        "outputId": "7aa04daa-282d-4299-8d9a-45311bd7f4fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the generated source and target sentences for the first sentence within corpus."
      ],
      "metadata": {
        "id": "QwEGWFv0-hvJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\r\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\r\n",
        "src_input = tensor[:, :-1]  \r\n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\r\n",
        "tgt_input = tensor[:, 1:]    \r\n",
        "\r\n",
        "print(src_input[0])\r\n",
        "print(tgt_input[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   2  290   28   94 4486    3    0    0    0    0    0    0    0    0]\n",
            "[ 290   28   94 4486    3    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sEDx5PH-WG5",
        "outputId": "0c1d0752-82b2-4ff4-9871-de378d3e0954"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The source is filled with 0 (\\<pad>) starting at 2 (\\<start>) and ending at 3 (\\<end>). However, the target doesn't start with a 2 and has the source shifted one space to the left."
      ],
      "metadata": {
        "id": "WfvBU3Ft-q_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Dataset and Test Dataset"
      ],
      "metadata": {
        "id": "vQyZIwXl5KKl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(src_input,tgt_input,test_size = 0.2, random_state = 15)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9JeMX9Mt_NDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "print(\"Source Train:\", x_train.shape)\r\n",
        "print(\"Target Train:\", y_train.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Train: (124810, 14)\n",
            "Target Train: (124810, 14)\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjZ_OH0oAx_Z",
        "outputId": "e5cf2a49-1764-4b86-9982-5fdb6d1cf834"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Model"
      ],
      "metadata": {
        "id": "Y05Miupy5NES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I added the `Dense` parameter to have `activation='softmax'`, and also added Dropout 0.2"
      ],
      "metadata": {
        "id": "QET2ni4NfjpL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "class TextGenerator(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\r\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\r\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\r\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size,  activation='softmax')  # added activation softmax\r\n",
        "        \r\n",
        "    def call(self, x):\r\n",
        "        out = self.embedding(x)\r\n",
        "        out = self.rnn_1(out)\r\n",
        "        out = self.rnn_2(out)\r\n",
        "        out = self.linear(out)\r\n",
        "        \r\n",
        "        return out\r\n",
        "    \r\n",
        "embedding_size = 256\r\n",
        "hidden_size = 2048\r\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ],
      "outputs": [],
      "metadata": {
        "id": "azclF9l3BYFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try Dropout to simplify data"
      ],
      "metadata": {
        "id": "muK9hVwZ_nmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "from keras.layers import Dropout\r\n",
        "\r\n",
        "model.add(Dropout(0.2))"
      ],
      "outputs": [],
      "metadata": {
        "id": "C1a_RolofN_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the dataset"
      ],
      "metadata": {
        "id": "TnAqJQwaIxOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of setting the `batch_size` separately with `tf.data.Dataset.from_tensor_slices()` and `dataset.batch`, I set `batch_size` to 256 as a parameter in the `model.fit()` function."
      ],
      "metadata": {
        "id": "jg4Sn784xiQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, I tried to add `validation_split = 0.25` here to evaluate the loss and check the accuracy at the end of each epoch. The train / test data is already splitted and I put them iinto `validation_data`. But this data is not shuffled."
      ],
      "metadata": {
        "id": "hBtyWUjgytsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I added `metrics=['accuracy']` to check the accuracy at each epoch, too."
      ],
      "metadata": {
        "id": "mUXMw2sH0-U4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "epochs = 10\r\n",
        "batch_size = 256\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True,\r\n",
        "    reduction='none'\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_test, y_test), epochs=epochs, validation_split=0.25)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "366/366 [==============================] - 337s 872ms/step - loss: 3.3262 - accuracy: 0.4920 - val_loss: 2.9613 - val_accuracy: 0.5207\n",
            "Epoch 2/10\n",
            "366/366 [==============================] - 316s 863ms/step - loss: 2.8181 - accuracy: 0.5314 - val_loss: 2.7372 - val_accuracy: 0.5381\n",
            "Epoch 3/10\n",
            "366/366 [==============================] - 316s 863ms/step - loss: 2.5831 - accuracy: 0.5490 - val_loss: 2.5931 - val_accuracy: 0.5535\n",
            "Epoch 4/10\n",
            "366/366 [==============================] - 315s 860ms/step - loss: 2.3559 - accuracy: 0.5698 - val_loss: 2.4770 - val_accuracy: 0.5689\n",
            "Epoch 5/10\n",
            "366/366 [==============================] - 316s 864ms/step - loss: 2.1192 - accuracy: 0.5973 - val_loss: 2.3870 - val_accuracy: 0.5846\n",
            "Epoch 6/10\n",
            "366/366 [==============================] - 315s 861ms/step - loss: 1.8798 - accuracy: 0.6311 - val_loss: 2.3181 - val_accuracy: 0.6009\n",
            "Epoch 7/10\n",
            "366/366 [==============================] - 325s 888ms/step - loss: 1.6482 - accuracy: 0.6708 - val_loss: 2.2695 - val_accuracy: 0.6169\n",
            "Epoch 8/10\n",
            "366/366 [==============================] - 315s 860ms/step - loss: 1.4406 - accuracy: 0.7110 - val_loss: 2.2484 - val_accuracy: 0.6292\n",
            "Epoch 9/10\n",
            "366/366 [==============================] - 324s 885ms/step - loss: 1.2660 - accuracy: 0.7486 - val_loss: 2.2433 - val_accuracy: 0.6382\n",
            "Epoch 10/10\n",
            "366/366 [==============================] - 315s 861ms/step - loss: 1.1371 - accuracy: 0.7774 - val_loss: 2.2587 - val_accuracy: 0.6441\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiwpTTlHBlkp",
        "outputId": "d006a2a4-68ee-4fb7-ad2d-50c58413b0ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TT.. the val_loss is 2.2587, slightly greater then my goal which is 2.2."
      ],
      "metadata": {
        "id": "PQaSxwpgAuix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This training is taking a lot of time. Literally, too slow:** it took more than 40 mins. I was curious about how I can reduce the slowness. (I added the number of hidden layers and I know this will make the train slow, but I want to improve this as possible.)   \n"
      ],
      "metadata": {
        "id": "kA1vLAaX9gNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Checkpoints\n",
        "And I found that I can **define a checkpoint to record all of the network weights to file each time an improvement in loss** is observed at the end of the epoch. (This means we can save some time if the computation is already done in the previous epoch!)   \n",
        "I will use the **best set of weights (lowest loss) **to the model."
      ],
      "metadata": {
        "id": "LY5YpkMT-poN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "from keras.callbacks import ModelCheckpoint\r\n",
        "\r\n",
        "# define the checkpoint\r\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]"
      ],
      "outputs": [],
      "metadata": {
        "id": "c24rWVcc-tb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model again"
      ],
      "metadata": {
        "id": "zSRV98ZsevKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "epochs = 10\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True,\r\n",
        "    reduction='none'\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(loss=loss, optimizer=optimizer)\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train, batch_size=256, validation_data=(x_test, y_test), epochs=epochs,)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "488/488 [==============================] - 239s 445ms/step - loss: 3.2466 - val_loss: 2.8590\n",
            "Epoch 2/10\n",
            "488/488 [==============================] - 233s 479ms/step - loss: 2.7114 - val_loss: 2.6014\n",
            "Epoch 3/10\n",
            "488/488 [==============================] - 234s 480ms/step - loss: 2.4223 - val_loss: 2.4245\n",
            "Epoch 4/10\n",
            "488/488 [==============================] - 235s 482ms/step - loss: 2.1313 - val_loss: 2.2865\n",
            "Epoch 5/10\n",
            "488/488 [==============================] - 235s 481ms/step - loss: 1.8473 - val_loss: 2.1858\n",
            "Epoch 6/10\n",
            "488/488 [==============================] - 235s 481ms/step - loss: 1.5902 - val_loss: 2.1180\n",
            "Epoch 7/10\n",
            "488/488 [==============================] - 235s 481ms/step - loss: 1.3770 - val_loss: 2.0847\n",
            "Epoch 8/10\n",
            "488/488 [==============================] - 235s 481ms/step - loss: 1.2137 - val_loss: 2.0783\n",
            "Epoch 9/10\n",
            "488/488 [==============================] - 235s 482ms/step - loss: 1.1032 - val_loss: 2.0883\n",
            "Epoch 10/10\n",
            "488/488 [==============================] - 235s 482ms/step - loss: 1.0400 - val_loss: 2.1068\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiwpTTlHBlkp",
        "outputId": "4c9b74ec-1a85-4af8-af95-9104c3550f9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, wih the `hidden_size` with 2048, I was able to got the `val_loss` of 2.1068."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting?"
      ],
      "metadata": {
        "id": "l3v4jyiU_kmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But it looks like that there is an **overfitting**. The `val_loss` was dropped to 2.0883 at the epoch 9/10 and then incremented to 2.1068 at the epoch 10/10   \r\n",
        "To reduce the overfitting, there are 3 options that I can think of.   \r\n",
        "1. Put more data into the train dataset.   \r\n",
        "2. Data Augmentation.   \r\n",
        "3. Simplify data with `dropout` to rduce overfittng by decreasiing the complexity of the model to prevent overfit.   \r\n",
        "\r\n",
        "However it seems like I need to work only with the given lyrics dataset, so I want to try the `dropout` method."
      ],
      "metadata": {
        "id": "0fDX6bqH8AAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate lyrics"
      ],
      "metadata": {
        "id": "VFm7hfeg1zcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is ready now, let's generate lyrics with the model!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\r\n",
        "    # Convert input init_sentence to tensor for testing purposes. \r\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\r\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\r\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\r\n",
        "\r\n",
        "    # Create a sentence by predicting a word\r\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\r\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\r\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\r\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\r\n",
        "    while True:\r\n",
        "        # 1\r\n",
        "        predict = model(test_tensor) \r\n",
        "        # 2\r\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \r\n",
        "        # 3 \r\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\r\n",
        "        # 4\r\n",
        "        if predict_word.numpy()[0] == end_token: break\r\n",
        "        if test_tensor.shape[1] >= max_len: break\r\n",
        "\r\n",
        "    generated = \"\"\r\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \r\n",
        "    for word_index in test_tensor[0].numpy():\r\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\r\n",
        "\r\n",
        "    return generated"
      ],
      "outputs": [],
      "metadata": {
        "id": "FHs88hCV1cZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i love you and the breeze that <unk> around you <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "23H3ESqi12ev",
        "outputId": "2465d7bd-3166-4bce-d976-29ff958eb291"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How sweet..."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> New York\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> new york city <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KN2EJjO7309J",
        "outputId": "e409c936-10cb-498a-cbde-f2494f5521a9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i want\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i want to get in the zone <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uF-l5tVx35r7",
        "outputId": "42342343-bf1c-46af-bba6-ee1071d3e4af"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> What can I\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> what can i do for you ? <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1NSZhiE038WN",
        "outputId": "fa3c65a1-59e8-4196-e53c-3fc0af89f59c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> What should I\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> what should i do , babe ? <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "chlpCllF4ECr",
        "outputId": "30f99499-c3c0-4b4a-8516-251e62c67ac4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is so sweet..!!! "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> Love\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> love me like you do <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TjbBNBQS4HMN",
        "outputId": "c5593c48-de35-47eb-8eca-3aee085b68fc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> You love\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> you love when i whine it <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nKQoqYJw4KLU",
        "outputId": "95d78031-babb-4f83-966a-b474ce83936d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> i hate the headlines and the weather <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wn3p2fQY4Pjz",
        "outputId": "f79361a3-a710-4c8f-fd29-70150ad1779b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> This evening\", max_len=20)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> this evening s too heavy , <end> '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i6f8p0Nv4STO",
        "outputId": "cd6bab9a-85ea-4d88-a6f3-2a9aa37197b4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lyrics that the model generated are so poetic and romantic. I guess this is because we trained the model with love songs, so our lyricist model is full of **LOVE**."
      ],
      "metadata": {
        "id": "XoEUM42L3zo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "1kB8ZoqE5xiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What I've learned and tried\r\n",
        "- I learned how to preprocess the text data to organize them with predictors. \r\n",
        "- Build a RNN model using LSTM.   \r\n",
        "- Fit the train data with embedding size and hidden layers size.\r\n",
        "- Tried adding **Checkpoints** to reduce the time to train the model.\r\n",
        "- Got an acceptable validation loss of the text generator model, lower then 2.2.\r\n",
        "    \r\n",
        "# Things that I learned - RNNs & LSTM networks\r\n",
        "- It works with **Sequential** data\r\n",
        "- It works greatly with **Short Contexts**. Simple RNN's prediction is dependent on all previous predictions and information learned from them. Though its effective for those short contexts.\r\n",
        "- The reason for the above dependency is becuase of **Vanishing Gradient**. RNN remembers things for a small duration of time. This is because for a conventional feed-forward neural netowrk, the weight updating that is applied on a particular layer is a multiple of the learning rate.\r\n",
        "- That is why we used RNN with **LSTM**, (Long Short-Term Memory Networks).\r\n",
        "- Disadvantage of LSTM: it takes to much time to train this simple model. (Hardware Constraint)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Ideas to Improve the Model\r\n",
        "1. Add dropout to the visible input layer and consider tuning the dropout percentage.\r\n",
        "2. Add `Bidirectional` LSTM layer\r\n",
        "3. Tune the batch size, try a batch size of 1 as a (very slow) baseline and larger sizes from there.\r\n",
        "4. Try a one hot encoded for the input sequences.\r\n",
        "5. Apply `ModelCheckpoint` appropriately. ( I was failed to apply this because I thought I had to change the model type to `Seuquential` to do this.)"
      ],
      "metadata": {
        "id": "8glnUQg-tDrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "- I got a hint from this article to use `Bidirection al LSTM`: https://www.programmersought.com/article/67438889091/"
      ],
      "metadata": {
        "id": "rPmcpKQg8w-i"
      }
    }
  ]
}